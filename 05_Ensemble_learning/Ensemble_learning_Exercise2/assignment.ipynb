{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dab637fa-8fc8-4db9-be68-11f72f35d1ef",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "\n",
    "You are a data scientist / AI engineer working on a binary classification problem. You have been provided with a dataset named **`\"mushroom_classification.csv\"`**, which includes various features of mushrooms to predict whether they are edible or poisonous. The dataset comprises the following columns:\n",
    "\n",
    "- `cap_diameter:` The diameter of the mushroom cap.\n",
    "- `cap_shape:` The shape of the mushroom cap, encoded as integers.\n",
    "- `gill_attachment:` The attachment of the gills, encoded as integers.\n",
    "- `gill_color:` The color of the gills, encoded as integers.\n",
    "- `stem_height:` The height of the mushroom stem.\n",
    "- `stem_width` The width of the mushroom stem.\n",
    "- `stem_color:` The color of the mushroom stem, encoded as integers.\n",
    "- `season:` The season when the mushroom was found, encoded as float.\n",
    "- `class:` The classification of the mushroom, where 0 indicates edible and 1 indicates poisonous.\n",
    "\n",
    "Your task is to use this dataset to build and evaluate a binary classification model to classify mushrooms as edible or poisonous. You will start with basic models and gradually move towards advanced models like Gradient Boosting. Finally, you will explore various parameters of the Gradient Boosting model to enhance performance.\n",
    "\n",
    "**Dataset credits:** Prisha Sawhney (https://www.kaggle.com/datasets/prishasawhney/mushroom-dataset/data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bd3443-1dfb-423e-a352-ed4c467635d6",
   "metadata": {},
   "source": [
    "**Import Necessary Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16910661-4e73-4474-a003-e7b98a391352",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b417318-3bea-4581-a1dc-9f049055a2c3",
   "metadata": {},
   "source": [
    "### Task 1: Data Preparation and Exploration\n",
    "\n",
    "1. Import the data from the `\"mushroom_classification.csv\"` file and store it in a variable df.\n",
    "2. Display the number of rows and columns in the dataset.\n",
    "3. Display the first few rows of the dataset to get an overview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a63849e-217e-4de0-acf4-b3bafc415424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cap_diameter</th>\n",
       "      <th>cap_shape</th>\n",
       "      <th>gill_attachment</th>\n",
       "      <th>gill_color</th>\n",
       "      <th>stem_height</th>\n",
       "      <th>stem_width</th>\n",
       "      <th>stem_color</th>\n",
       "      <th>season</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1372</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>3.807467</td>\n",
       "      <td>1545</td>\n",
       "      <td>11</td>\n",
       "      <td>1.804273</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1461</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>3.807467</td>\n",
       "      <td>1557</td>\n",
       "      <td>11</td>\n",
       "      <td>1.804273</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1371</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>3.612496</td>\n",
       "      <td>1566</td>\n",
       "      <td>11</td>\n",
       "      <td>1.804273</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1261</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>3.787572</td>\n",
       "      <td>1566</td>\n",
       "      <td>11</td>\n",
       "      <td>1.804273</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1305</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>3.711971</td>\n",
       "      <td>1464</td>\n",
       "      <td>11</td>\n",
       "      <td>0.943195</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cap_diameter  cap_shape  gill_attachment  gill_color  stem_height  \\\n",
       "0          1372          2                2          10     3.807467   \n",
       "1          1461          2                2          10     3.807467   \n",
       "2          1371          2                2          10     3.612496   \n",
       "3          1261          6                2          10     3.787572   \n",
       "4          1305          6                2          10     3.711971   \n",
       "\n",
       "   stem_width  stem_color    season  class  \n",
       "0        1545          11  1.804273      1  \n",
       "1        1557          11  1.804273      1  \n",
       "2        1566          11  1.804273      1  \n",
       "3        1566          11  1.804273      1  \n",
       "4        1464          11  0.943195      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Import the data from the \"mushroom_classification.csv\" file and store it in a variable 'df'\n",
    "df = pd.read_csv(\"mushroom_classification.csv\")\n",
    "\n",
    "# Step 2: Display the number of rows and columns in the dataset\n",
    "\n",
    "\n",
    "# Step 3: Display the first few rows of the dataset to get an overview\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd93c501-4716-4585-9cdd-d1306e7ef8f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54035, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155cd929-a553-4a27-9cc0-7ee4115bcfea",
   "metadata": {},
   "source": [
    "### Task 2: Exploratory Data Analysis (EDA)\n",
    "\n",
    "1. Perform a group-by operation on the target class and calculate the mean of the following features: `cap_diameter, stem_height, and stem_width`.\n",
    "2. Visualize the distribution of these features using box plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa0539e2-7d51-4c43-8831-4484fc0b13b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cap_diameter</th>\n",
       "      <th>stem_height</th>\n",
       "      <th>stem_width</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>633.064696</td>\n",
       "      <td>0.627374</td>\n",
       "      <td>1208.915189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>513.236293</td>\n",
       "      <td>0.867251</td>\n",
       "      <td>921.516563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cap_diameter  stem_height   stem_width\n",
       "class                                        \n",
       "0        633.064696     0.627374  1208.915189\n",
       "1        513.236293     0.867251   921.516563"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Perform a group-by operation on the target class and calculate the mean of specific features\n",
    "df.groupby(\"class\")[[\"cap_diameter\" , \"stem_height\" , \"stem_width\"]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "800d3c15-32bf-4a3c-b0c2-59d2078d0fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Visualize the distribution of these features using box plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2ab061-c25f-4441-adb0-19f0ddebfaa8",
   "metadata": {},
   "source": [
    "### Task 3: Model Training Using Basic Models\n",
    "\n",
    "1. Select the features `(cap_diameter, cap_shape, gill_attachment, gill_color, stem_height, stem_width, stem_color, season)` and the target variable `(class)` for modeling.\n",
    "2. Split the data into training and test sets with a test size of 25%.\n",
    "3. Initialize and train a Logistic Regression model using the training data.\n",
    "4. Make predictions on the test set using the trained model.\n",
    "5. Evaluate the model using a classification report and print the report.\n",
    "6. Initialize and train a Decision Tree Classifier model using the training data.\n",
    "7. Make predictions on the test set using the trained model.\n",
    "8. Evaluate the model using a classification report and print the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca28e050-af95-48a0-8e69-c59e438a37e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Select the features and target variable for modeling\n",
    "X = df.drop([\"class\"] , axis = 1)\n",
    "y = df[\"class\"]\n",
    "\n",
    "# Step 2: Split the data into training and test sets with a test size of 25%\n",
    "X_train , X_test , y_train , y_test = train_test_split(X , y , test_size = 0.25 , random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5802ce9b-2018-43ed-9869-0bec1077405b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: continuous-multioutput. Maybe you are trying to fit a classifier, which expects discrete classes on a regression target with continuous values.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DecisionTreeClassifier\n\u001b[32m      3\u001b[39m dt = DecisionTreeClassifier(criterion=\u001b[33m'\u001b[39m\u001b[33mgini\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mdt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Step 4: Make predictions on the test set using the trained model\u001b[39;00m\n\u001b[32m      8\u001b[39m y_predict  = dt.predict(x_test)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\tree\\_classes.py:1024\u001b[39m, in \u001b[36mDecisionTreeClassifier.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input)\u001b[39m\n\u001b[32m    993\u001b[39m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    994\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m, check_input=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    995\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[32m    996\u001b[39m \n\u001b[32m    997\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1021\u001b[39m \u001b[33;03m        Fitted estimator.\u001b[39;00m\n\u001b[32m   1022\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1028\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1030\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\tree\\_classes.py:294\u001b[39m, in \u001b[36mBaseDecisionTree._fit\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28mself\u001b[39m.n_outputs_ = y.shape[\u001b[32m1\u001b[39m]\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_classification:\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m     \u001b[43mcheck_classification_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    295\u001b[39m     y = np.copy(y)\n\u001b[32m    297\u001b[39m     \u001b[38;5;28mself\u001b[39m.classes_ = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\multiclass.py:221\u001b[39m, in \u001b[36mcheck_classification_targets\u001b[39m\u001b[34m(y)\u001b[39m\n\u001b[32m    213\u001b[39m y_type = type_of_target(y, input_name=\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[32m    215\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbinary\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    216\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmulticlass\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    219\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmultilabel-sequences\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    220\u001b[39m ]:\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    222\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnknown label type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Maybe you are trying to fit a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    223\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mclassifier, which expects discrete classes on a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    224\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mregression target with continuous values.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    225\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Unknown label type: continuous-multioutput. Maybe you are trying to fit a classifier, which expects discrete classes on a regression target with continuous values."
     ]
    }
   ],
   "source": [
    "# Step 3: Initialize and train a Logistic Regression model using the training data\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(criterion='gini')\n",
    "\n",
    "dt.fit(X_train , X_test)\n",
    "\n",
    "# Step 4: Make predictions on the test set using the trained model\n",
    "y_predict  = dt.predict(x_test)\n",
    "\n",
    "# Step 5: Evaluate the model using a classification report and print the report\n",
    "print(Classification_report(y_predict , y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0ee4a78-b5e3-4872-8b59-9ec37d508b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Initialize and train a Decision Tree Classifier model using the training data\n",
    "\n",
    "\n",
    "# Step 7: Make predictions on the test set using the trained model\n",
    "\n",
    "\n",
    "# Step 8: Evaluate the model using a classification report and print the report\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bf4ec0-b8d5-41da-b636-5f06b0b66a2d",
   "metadata": {},
   "source": [
    "### Task 4: Model Training Using Gradient Boosting Classifier\n",
    "\n",
    "1. Initialize and train a Gradient Boosting Classifier model using the training data.\n",
    "2. Make predictions on the test set using the trained model.\n",
    "3. Evaluate the model using a classification report and print the report.\n",
    "4. Calculate and display the feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "242e67dd-e3e9-444b-a6e7-5352767cc229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize and train a Gradient Boosting Classifier model using the training data\n",
    "\n",
    "\n",
    "# Step 2: Make predictions on the test set using the trained model\n",
    "\n",
    "\n",
    "# Step 3: Evaluate the model using a classification report and print the report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05020492-f728-4a92-bd2b-5b5e30413738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Calculate and display the feature importances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc96c15-d72c-4a4c-9c40-9ae93a41de00",
   "metadata": {},
   "source": [
    "### Task 5: Exploring Various Parameters in Gradient Boosting Classifier\n",
    "\n",
    "1. Train a Gradient Boosting model with the following parameters:\n",
    "    - learning_rate = 0.05\n",
    "    - n_estimators = 150\n",
    "    - max_depth=4\n",
    "    - min_samples_split = 3\n",
    "    - min_samples_leaf = 2\n",
    "\n",
    "Learn about these parameters here: [scikit-learn GradientBoostingClassifier Parameters](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "2. Evaluate the model using a classification report and print the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56402d3f-b721-4c94-bc8b-a724b27a5b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Train a Gradient Boosting model with specified parameters\n",
    "\n",
    "\n",
    "# Step 2: Make predictions on the test set using the trained model\n",
    "\n",
    "\n",
    "# Step 3: Evaluate the model using a classification report and print the report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c5ce1a-834f-4b50-ae32-d43162687c86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
